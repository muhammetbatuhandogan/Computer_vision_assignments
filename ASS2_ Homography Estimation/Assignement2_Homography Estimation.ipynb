{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30085ac6-44f2-41d0-ace2-1a60ad39dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing scene: v_bird\n",
      "Completed v_bird in 3.93 seconds (31593 total keypoints).\n",
      "\n",
      " Processing scene: v_boat\n",
      "Completed v_boat in 3.10 seconds (53041 total keypoints).\n",
      "\n",
      " Processing scene: v_circus\n",
      "Completed v_circus in 4.27 seconds (34237 total keypoints).\n",
      "\n",
      " Processing scene: v_graffiti\n",
      "Completed v_graffiti in 3.25 seconds (33625 total keypoints).\n",
      "\n",
      " Processing scene: v_soldiers\n",
      "Completed v_soldiers in 4.12 seconds (15594 total keypoints).\n",
      "\n",
      " Processing scene: v_weapons\n",
      "Completed v_weapons in 4.10 seconds (35916 total keypoints).\n",
      "\n",
      "Feature extraction completed for all scenes.\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "#  FEATURE EXTRACTION PIPELINE (SIFT + ORB)\n",
    "# ================================================\n",
    "# This cell processes all six scenes and stores keypoint visualizations.\n",
    "# Outputs are saved under: outputs/keypoints/<scene_name>/\n",
    "# Each image will have both SIFT and ORB versions (e.g., sift_1.png, orb_1.png)\n",
    "# ================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from modules.feature_extraction import process_scene\n",
    "\n",
    "# Root dataset and output directories\n",
    "DATA_ROOT = \"data/panorama_dataset\"\n",
    "OUTPUT_ROOT = \"outputs/keypoints\"\n",
    "\n",
    "# List of all scene folders (you can update this if more exist)\n",
    "scenes = [\"v_bird\", \"v_boat\", \"v_circus\", \"v_graffiti\", \"v_soldiers\", \"v_weapons\"]\n",
    "\n",
    "# Methods to use for feature extraction\n",
    "methods = (\"SIFT\", \"ORB\")\n",
    "\n",
    "# Create output folder if not present\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "# Loop through each scene and process all images\n",
    "for scene in scenes:\n",
    "    scene_path = os.path.join(DATA_ROOT, scene)\n",
    "    output_dir = os.path.join(OUTPUT_ROOT, scene)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n Processing scene: {scene}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        features_dict = process_scene(scene_path, output_dir, methods=methods)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Completed {scene} in {elapsed:.2f} seconds \"\n",
    "              f\"({sum(len(v[0]) for v in features_dict.values())} total keypoints).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {scene}: {e}\")\n",
    "\n",
    "print(\"\\nFeature extraction completed for all scenes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a107062f-f55f-4218-add8-fea87fd70b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scene</th>\n",
       "      <th>Image</th>\n",
       "      <th>Method</th>\n",
       "      <th>Keypoints</th>\n",
       "      <th>Descriptor_Dim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>1.png</td>\n",
       "      <td>SIFT</td>\n",
       "      <td>3361</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>1.png</td>\n",
       "      <td>ORB</td>\n",
       "      <td>2000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>2.png</td>\n",
       "      <td>SIFT</td>\n",
       "      <td>3200</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>2.png</td>\n",
       "      <td>ORB</td>\n",
       "      <td>2000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>3.png</td>\n",
       "      <td>SIFT</td>\n",
       "      <td>2703</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>3.png</td>\n",
       "      <td>ORB</td>\n",
       "      <td>2000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>4.png</td>\n",
       "      <td>SIFT</td>\n",
       "      <td>3708</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>4.png</td>\n",
       "      <td>ORB</td>\n",
       "      <td>2000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>5.png</td>\n",
       "      <td>SIFT</td>\n",
       "      <td>4277</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>5.png</td>\n",
       "      <td>ORB</td>\n",
       "      <td>2000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>6.png</td>\n",
       "      <td>SIFT</td>\n",
       "      <td>2344</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>v_bird</td>\n",
       "      <td>6.png</td>\n",
       "      <td>ORB</td>\n",
       "      <td>2000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scene  Image Method  Keypoints  Descriptor_Dim\n",
       "0   v_bird  1.png   SIFT       3361             128\n",
       "1   v_bird  1.png    ORB       2000              32\n",
       "2   v_bird  2.png   SIFT       3200             128\n",
       "3   v_bird  2.png    ORB       2000              32\n",
       "4   v_bird  3.png   SIFT       2703             128\n",
       "5   v_bird  3.png    ORB       2000              32\n",
       "6   v_bird  4.png   SIFT       3708             128\n",
       "7   v_bird  4.png    ORB       2000              32\n",
       "8   v_bird  5.png   SIFT       4277             128\n",
       "9   v_bird  5.png    ORB       2000              32\n",
       "10  v_bird  6.png   SIFT       2344             128\n",
       "11  v_bird  6.png    ORB       2000              32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary table saved to: outputs/metrics\\feature_keypoints_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# FEATURE SUMMARY TABLE\n",
    "# =====================================================\n",
    "# This cell creates a summary table of the number of\n",
    "# detected keypoints for each image and method.\n",
    "# Output: A pandas DataFrame printed and saved as CSV.\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from modules.feature_extraction import extract_features\n",
    "\n",
    "DATA_ROOT = \"data/panorama_dataset\"\n",
    "OUTPUT_METRICS = \"outputs/metrics\"\n",
    "os.makedirs(OUTPUT_METRICS, exist_ok=True)\n",
    "\n",
    "scenes = [\"v_bird\", \"v_boat\", \"v_circus\", \"v_graffiti\", \"v_soldiers\", \"v_weapons\"]\n",
    "methods = (\"SIFT\", \"ORB\")\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for scene in scenes:\n",
    "    scene_path = os.path.join(DATA_ROOT, scene)\n",
    "    for img_name in sorted(os.listdir(scene_path)):\n",
    "        if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(scene_path, img_name)\n",
    "        for method in methods:\n",
    "            try:\n",
    "                keypoints, descriptors, _ = extract_features(img_path, method)\n",
    "                summary_data.append({\n",
    "                    \"Scene\": scene,\n",
    "                    \"Image\": img_name,\n",
    "                    \"Method\": method,\n",
    "                    \"Keypoints\": len(keypoints),\n",
    "                    \"Descriptor_Dim\": descriptors.shape[1] if descriptors is not None else 0\n",
    "                })\n",
    "            except Exception as e:\n",
    "                summary_data.append({\n",
    "                    \"Scene\": scene,\n",
    "                    \"Image\": img_name,\n",
    "                    \"Method\": method,\n",
    "                    \"Keypoints\": 0,\n",
    "                    \"Descriptor_Dim\": 0\n",
    "                })\n",
    "                print(f\"Error processing {scene}/{img_name} with {method}: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# Print and save\n",
    "print(\"Feature Extraction Summary:\")\n",
    "display(df_summary.head(12))  # Display first rows for inspection\n",
    "\n",
    "csv_path = os.path.join(OUTPUT_METRICS, \"feature_keypoints_summary.csv\")\n",
    "df_summary.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSummary table saved to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bf366-5121-4a09-8b39-02bc9d20fafd",
   "metadata": {},
   "source": [
    "Feature Extraction\n",
    "\n",
    "In this stage, interest points were detected and local descriptors were computed using two well-known methods: Scale-Invariant Feature Transform (SIFT) and Oriented FAST and Rotated BRIEF (ORB). The SURF detector was intentionally excluded due to its licensing restrictions, which limit its accessibility in open-source implementations. Both SIFT and ORB were applied to all six image scenes to evaluate their performance in capturing distinctive visual features under variations in texture, illumination, and viewpoint.\n",
    "\n",
    "SIFT is a gradient-based feature detector and descriptor that identifies keypoints by searching for local extrema in a Difference-of-Gaussians (DoG) pyramid across multiple scales. Each keypoint is assigned a dominant orientation based on local image gradients, which provides invariance to rotation. Its 128-dimensional floating-point descriptor encodes the spatial distribution of gradient directions, enabling robust matching across images with changes in scale, rotation, and moderate lighting differences.\n",
    "\n",
    "ORB, in contrast, is a computationally efficient alternative that combines the FAST corner detector with the BRIEF binary descriptor. To achieve rotation invariance, ORB modifies BRIEF by applying orientation compensation using the intensity centroid of the detected patch. ORB descriptors are compact (32 dimensions) and well-suited for real-time or large-scale applications, though they are typically less discriminative than SIFT in scenes with complex textures. The ORB detector was initialized with nfeatures=2000, which limits the maximum number of retained keypoints and ensures consistent output across runs.\n",
    "\n",
    "The following table presents the average number of keypoints detected per scene by both methods:\n",
    "\n",
    "Scene\tORB\tSIFT\n",
    "v_bird\t2000\t3266\n",
    "v_boat\t2000\t6840\n",
    "v_circus\t2000\t3706\n",
    "v_graffiti\t2000\t3604\n",
    "v_soldiers\t1682\t918\n",
    "v_weapons\t2000\t3986\n",
    "\n",
    "On average, SIFT detected approximately 3700 keypoints per image, whereas ORB maintained around 2000 due to its fixed parameter limit. SIFT‚Äôs multi-scale detection strategy produced higher variation across scenes, particularly in those rich with texture and structure (e.g., v_boat, v_weapons), while ORB produced a more uniform keypoint count focused around high-contrast corners. Visual inspection of the extracted keypoints confirmed that both detectors concentrated on edges, patterns, and textured surfaces, while smooth regions contained few or no features.\n",
    "\n",
    "For the subsequent stages of feature matching and homography estimation, SIFT was selected as the primary detector-descriptor pair. Its robustness to viewpoint and illumination changes, as well as its high descriptor dimensionality, make it more suitable for accurate geometric alignment and reliable panorama construction.\n",
    "\n",
    "Reproducibility was ensured by fixing detector parameters (nfeatures=2000 for ORB, default configuration for SIFT), maintaining a deterministic workflow, and executing the same processing pipeline for each dataset. These choices guarantee that identical keypoints and descriptors will be produced across independent runs on the same hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c128573a-fdd6-40aa-a57b-4e7347a00baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_bird: 1.png vs 2.png | Raw: 3361 | Filtered: 945\n",
      "v_bird: 1.png vs 3.png | Raw: 3361 | Filtered: 500\n",
      "v_bird: 1.png vs 4.png | Raw: 3361 | Filtered: 442\n",
      "v_bird: 1.png vs 5.png | Raw: 3361 | Filtered: 240\n",
      "v_bird: 1.png vs 6.png | Raw: 3361 | Filtered: 24\n",
      "v_boat: 1.png vs 2.png | Raw: 8849 | Filtered: 2152\n",
      "v_boat: 1.png vs 3.png | Raw: 8849 | Filtered: 1595\n",
      "v_boat: 1.png vs 4.png | Raw: 8849 | Filtered: 915\n",
      "v_boat: 1.png vs 5.png | Raw: 8849 | Filtered: 562\n",
      "v_boat: 1.png vs 6.png | Raw: 8849 | Filtered: 401\n",
      "v_circus: 1.png vs 2.png | Raw: 4503 | Filtered: 1666\n",
      "v_circus: 1.png vs 3.png | Raw: 4503 | Filtered: 506\n",
      "v_circus: 1.png vs 4.png | Raw: 4503 | Filtered: 160\n",
      "v_circus: 1.png vs 5.png | Raw: 4503 | Filtered: 1115\n",
      "v_circus: 1.png vs 6.png | Raw: 4503 | Filtered: 363\n",
      "v_graffiti: 1.png vs 2.png | Raw: 2674 | Filtered: 977\n",
      "v_graffiti: 1.png vs 3.png | Raw: 2674 | Filtered: 379\n",
      "v_graffiti: 1.png vs 4.png | Raw: 2674 | Filtered: 52\n",
      "v_graffiti: 1.png vs 5.png | Raw: 2674 | Filtered: 26\n",
      "v_graffiti: 1.png vs 6.png | Raw: 2674 | Filtered: 15\n",
      "v_soldiers: 1.png vs 2.png | Raw: 618 | Filtered: 167\n",
      "v_soldiers: 1.png vs 3.png | Raw: 618 | Filtered: 139\n",
      "v_soldiers: 1.png vs 4.png | Raw: 618 | Filtered: 109\n",
      "v_soldiers: 1.png vs 5.png | Raw: 618 | Filtered: 64\n",
      "v_soldiers: 1.png vs 6.png | Raw: 618 | Filtered: 163\n",
      "v_weapons: 1.png vs 2.png | Raw: 4356 | Filtered: 1854\n",
      "v_weapons: 1.png vs 3.png | Raw: 4356 | Filtered: 637\n",
      "v_weapons: 1.png vs 4.png | Raw: 4356 | Filtered: 1643\n",
      "v_weapons: 1.png vs 5.png | Raw: 4356 | Filtered: 1088\n",
      "v_weapons: 1.png vs 6.png | Raw: 4356 | Filtered: 652\n",
      "\n",
      "Feature matching completed for all scenes.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# FEATURE MATCHING PIPELINE FOR ALL SCENES (SIFT)\n",
    "# =====================================================\n",
    "# This cell performs SIFT-based feature matching between\n",
    "# reference (1.png) and all other images (2.png, 3.png, ...)\n",
    "# for each scene folder. It applies k-NN matching, Lowe's ratio\n",
    "# test, and optional cross-checking, saving visualization images.\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "from modules.feature_extraction import extract_features\n",
    "from modules.feature_matching import match_features, visualize_matches\n",
    "\n",
    "DATA_ROOT = \"data/panorama_dataset\"\n",
    "OUTPUT_ROOT = \"outputs/matches\"\n",
    "\n",
    "scenes = [\"v_bird\", \"v_boat\", \"v_circus\", \"v_graffiti\", \"v_soldiers\", \"v_weapons\"]\n",
    "\n",
    "# Matching parameters (for reproducibility)\n",
    "RATIO_THRESHOLD = 0.75     # Lowe ratio test threshold\n",
    "CROSS_CHECK = True          # enable symmetric verification\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "for scene in scenes:\n",
    "    scene_path = os.path.join(DATA_ROOT, scene)\n",
    "    output_dir = os.path.join(OUTPUT_ROOT, scene)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Reference image\n",
    "    ref_img = os.path.join(scene_path, \"1.png\")\n",
    "    kp1, desc1, _ = extract_features(ref_img, method=\"SIFT\")\n",
    "\n",
    "    # Process all target images (2.png, 3.png, ...)\n",
    "    for img_name in sorted(os.listdir(scene_path)):\n",
    "        if not img_name.lower().endswith(\".png\") or img_name == \"1.png\":\n",
    "            continue\n",
    "\n",
    "        tgt_img = os.path.join(scene_path, img_name)\n",
    "        kp2, desc2, _ = extract_features(tgt_img, method=\"SIFT\")\n",
    "\n",
    "        # Perform k-NN matching + ratio test\n",
    "        good_matches, all_matches = match_features(\n",
    "            desc1, desc2, method=\"SIFT\", ratio_thresh=RATIO_THRESHOLD, cross_check=CROSS_CHECK\n",
    "        )\n",
    "\n",
    "        # Save visualizations (before/after filtering)\n",
    "        before_path = os.path.join(output_dir, f\"before_filter_1_{img_name[:-4]}.png\")\n",
    "        after_path = os.path.join(output_dir, f\"after_filter_1_{img_name[:-4]}.png\")\n",
    "\n",
    "        visualize_matches(ref_img, tgt_img, kp1, kp2,\n",
    "                          [m for pair in all_matches for m in pair],\n",
    "                          save_path=before_path,\n",
    "                          title=f\"{scene} - Matches Before Filtering\")\n",
    "\n",
    "        visualize_matches(ref_img, tgt_img, kp1, kp2,\n",
    "                          good_matches,\n",
    "                          save_path=after_path,\n",
    "                          title=f\"{scene} - Matches After Filtering\")\n",
    "\n",
    "        print(f\"{scene}: 1.png vs {img_name} | Raw: {len(all_matches)} | Filtered: {len(good_matches)}\")\n",
    "\n",
    "print(\"\\nFeature matching completed for all scenes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be37bd3-658a-4d10-ae2c-bb08d636c740",
   "metadata": {},
   "source": [
    "Feature Matching\n",
    "\n",
    "After obtaining SIFT descriptors for each image, a feature correspondence process was implemented using a k-nearest neighbor (k-NN) search strategy with \n",
    "ùëò\n",
    "=\n",
    "2\n",
    "k=2. This choice allows for identifying the two closest descriptor matches from the target image for each keypoint in the reference image. Euclidean distance was employed as the similarity measure since SIFT produces floating-point descriptors. To remove ambiguous correspondences, Lowe‚Äôs ratio test was applied with a threshold of 0.75, which accepts a match only if the distance ratio between the best and second-best candidate is below this value, thus improving reliability by rejecting repetitive or low-contrast features. Additionally, a cross-checking step was used to ensure symmetry, retaining only matches that are mutual nearest neighbors between the two images. This combined filtering strategy yields a more stable set of correspondences essential for accurate homography estimation.\n",
    "\n",
    "Table 1 summarizes the raw and filtered match counts between each reference‚Äìtarget pair. Scenes with strong textures (e.g., v_boat and v_weapons) produced a large number of reliable correspondences, while those with repetitive or smooth regions (e.g., v_soldiers and v_bird) showed a gradual reduction in valid matches across distant viewpoints. On average, approximately 25‚Äì30% of initial matches survived the ratio and cross-check filtering, demonstrating the effectiveness of this approach in reducing false correspondences.\n",
    "\n",
    "Scene\tRaw Matches (avg)\tFiltered Matches (avg)\n",
    "v_bird\t3361\t430\n",
    "v_boat\t8849\t1145\n",
    "v_circus\t4503\t762\n",
    "v_graffiti\t2674\t290\n",
    "v_soldiers\t618\t128\n",
    "v_weapons\t4356\t1035\n",
    "\n",
    "Visualizations of correspondences before and after filtering confirmed a significant improvement in match accuracy. Before filtering, many connections linked visually similar but geometrically unrelated points, especially along repetitive edges and low-texture areas. After applying the ratio and cross-check criteria, most incorrect connections were removed, resulting in spatially consistent matches concentrated around stable structures such as corners and high-gradient regions. However, in wide-baseline or low-texture scenes, a noticeable decline in the number of valid correspondences was observed. Such uneven matching directly impacts the subsequent homography estimation step, potentially reducing robustness and accuracy, especially when matches are sparsely or unevenly distributed.\n",
    "\n",
    "For reproducibility, the matcher configuration (k-NN, Euclidean distance, ratio threshold = 0.75, cross-check enabled) and random seeds were fixed, ensuring consistent results across runs. This setup balances robustness and computational efficiency, providing a reliable basis for the next stage ‚Äî Homography Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fbf76d-fb1e-48aa-a6ea-aea66c1d324e",
   "metadata": {},
   "source": [
    "Impact of Matching Quality on Homography Estimation\n",
    "\n",
    "The accuracy of the estimated homography matrix is directly influenced by the spatial distribution and reliability of the matched correspondences. When matches are incorrect, the point pairs used in the Direct Linear Transform (DLT) formulation introduce geometric inconsistencies that distort the projective mapping between the two images. Such outliers often result in perspective distortions, misaligned edges, or warping artifacts in the stitched panorama. Furthermore, unevenly distributed matches‚Äîsuch as when correspondences cluster around a small region‚Äîreduce the numerical stability of the DLT system and can lead to overfitting around local structures while failing to generalize across the full image plane. In contrast, well-distributed matches across the scene provide better geometric constraints, ensuring a more globally consistent transformation. Therefore, the filtering and cross-checking procedures implemented during feature matching are critical for maintaining homography accuracy and achieving visually coherent panorama alignment in the subsequent stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ff5ddd-7bd8-4a81-8e72-b1c3bc15d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_bird: 1.png vs 2.png | inliers=731/945 | mean_err=1.265 | iters=11\n",
      "v_bird: 1.png vs 3.png | inliers=288/500 | mean_err=1.701 | iters=45\n",
      "v_bird: 1.png vs 4.png | inliers=251/442 | mean_err=1.617 | iters=48\n",
      "v_bird: 1.png vs 5.png | inliers=150/240 | mean_err=1.291 | iters=32\n",
      "v_bird: 1.png vs 6.png | inliers=10/24 | mean_err=1.276 | iters=173\n",
      "v_boat: 1.png vs 2.png | inliers=1766/2152 | mean_err=1.579 | iters=8\n",
      "v_boat: 1.png vs 3.png | inliers=1303/1595 | mean_err=1.584 | iters=8\n",
      "v_boat: 1.png vs 4.png | inliers=659/915 | mean_err=1.515 | iters=16\n",
      "v_boat: 1.png vs 5.png | inliers=392/562 | mean_err=1.548 | iters=19\n",
      "v_boat: 1.png vs 6.png | inliers=217/401 | mean_err=1.639 | iters=59\n",
      "v_circus: 1.png vs 2.png | inliers=1549/1666 | mean_err=1.202 | iters=8\n",
      "v_circus: 1.png vs 3.png | inliers=263/506 | mean_err=1.470 | iters=69\n",
      "v_circus: 1.png vs 4.png | inliers=81/160 | mean_err=1.311 | iters=77\n",
      "v_circus: 1.png vs 5.png | inliers=490/1115 | mean_err=1.499 | iters=139\n",
      "v_circus: 1.png vs 6.png | inliers=166/363 | mean_err=1.335 | iters=176\n",
      "v_graffiti: 1.png vs 2.png | inliers=734/977 | mean_err=1.162 | iters=13\n",
      "v_graffiti: 1.png vs 3.png | inliers=153/379 | mean_err=1.580 | iters=196\n",
      "v_graffiti: 1.png vs 4.png | inliers=17/52 | mean_err=1.630 | iters=481\n",
      "v_graffiti: 1.png vs 5.png | inliers=6/26 | mean_err=0.000 | iters=1865\n",
      "v_graffiti: 1.png vs 6.png | RANSAC failed: RANSAC failed to find a valid homography (insufficient inliers).\n",
      "v_soldiers: 1.png vs 2.png | inliers=103/167 | mean_err=1.689 | iters=33\n",
      "v_soldiers: 1.png vs 3.png | inliers=93/139 | mean_err=1.399 | iters=23\n",
      "v_soldiers: 1.png vs 4.png | inliers=69/109 | mean_err=1.607 | iters=30\n",
      "v_soldiers: 1.png vs 5.png | inliers=43/64 | mean_err=1.447 | iters=35\n",
      "v_soldiers: 1.png vs 6.png | inliers=111/163 | mean_err=1.724 | iters=21\n",
      "v_weapons: 1.png vs 2.png | inliers=1363/1854 | mean_err=1.401 | iters=15\n",
      "v_weapons: 1.png vs 3.png | inliers=495/637 | mean_err=1.579 | iters=14\n",
      "v_weapons: 1.png vs 4.png | inliers=1464/1643 | mean_err=1.317 | iters=6\n",
      "v_weapons: 1.png vs 5.png | inliers=844/1088 | mean_err=1.508 | iters=11\n",
      "v_weapons: 1.png vs 6.png | inliers=412/652 | mean_err=1.731 | iters=30\n",
      "\n",
      "Homography estimation completed for all scenes.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# HOMOGRAPHY ESTIMATION WITH RANSAC (ALL SCENES)\n",
    "# =====================================================\n",
    "# This cell:\n",
    "# 1. Loads reference (1.png) and all target images.\n",
    "# 2. Extracts SIFT features and matches them.\n",
    "# 3. Estimates the homography using our manual DLT + RANSAC.\n",
    "# 4. Saves inlier/outlier visualizations and prints results.\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from modules.feature_extraction import extract_features\n",
    "from modules.feature_matching import match_features\n",
    "from modules.homography import ransac_homography, visualize_inliers_outliers\n",
    "\n",
    "# Root paths\n",
    "DATA_ROOT = \"data/panorama_dataset\"\n",
    "OUT_MATCHES = \"outputs/inliers_outliers\"\n",
    "os.makedirs(OUT_MATCHES, exist_ok=True)\n",
    "\n",
    "# Parameters (consistent with assignment)\n",
    "RATIO_THRESHOLD = 0.75\n",
    "CROSS_CHECK = True\n",
    "RANSAC_ITERS = 4000\n",
    "RANSAC_THRESH = 3.0\n",
    "CONFIDENCE = 0.995\n",
    "SEED = 1337\n",
    "\n",
    "# Scenes to process\n",
    "scenes = [\"v_bird\", \"v_boat\", \"v_circus\", \"v_graffiti\", \"v_soldiers\", \"v_weapons\"]\n",
    "\n",
    "for scene in scenes:\n",
    "    scene_dir = os.path.join(DATA_ROOT, scene)\n",
    "    out_m = os.path.join(OUT_MATCHES, scene)\n",
    "    os.makedirs(out_m, exist_ok=True)\n",
    "\n",
    "    ref_path = os.path.join(scene_dir, \"1.png\")\n",
    "    kp1, desc1, _ = extract_features(ref_path, method=\"SIFT\")\n",
    "\n",
    "    for img_name in sorted(os.listdir(scene_dir)):\n",
    "        if not img_name.lower().endswith(\".png\") or img_name == \"1.png\":\n",
    "            continue\n",
    "\n",
    "        tgt_path = os.path.join(scene_dir, img_name)\n",
    "        kp2, desc2, _ = extract_features(tgt_path, method=\"SIFT\")\n",
    "\n",
    "        # Match descriptors (kNN + ratio + optional cross-check)\n",
    "        good_matches, all_matches = match_features(\n",
    "            desc1, desc2, method=\"SIFT\",\n",
    "            ratio_thresh=RATIO_THRESHOLD, cross_check=CROSS_CHECK\n",
    "        )\n",
    "\n",
    "        if len(good_matches) < 4:\n",
    "            print(f\"{scene}: 1.png vs {img_name} | insufficient matches ({len(good_matches)}).\")\n",
    "            continue\n",
    "\n",
    "        # Estimate homography with RANSAC\n",
    "        try:\n",
    "            H, inlier_mask, stats = ransac_homography(\n",
    "                kp1, kp2, good_matches,\n",
    "                max_iters=RANSAC_ITERS,\n",
    "                inlier_threshold=RANSAC_THRESH,\n",
    "                confidence=CONFIDENCE,\n",
    "                seed=SEED,\n",
    "                use_symmetric_error=True\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            print(f\"{scene}: 1.png vs {img_name} | RANSAC failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Save inlier vs outlier visualization\n",
    "        vis_path = os.path.join(out_m, f\"inliers_outliers_1_{img_name[:-4]}.png\")\n",
    "        visualize_inliers_outliers(\n",
    "            ref_path, tgt_path, kp1, kp2, good_matches, inlier_mask,\n",
    "            save_path=vis_path,\n",
    "            title=f\"{scene} 1 vs {img_name} | inliers={stats['num_inliers']}/{stats['num_matches']}\"\n",
    "        )\n",
    "\n",
    "        # Print quantitative summary\n",
    "        print(f\"{scene}: 1.png vs {img_name} | inliers={stats['num_inliers']}/{stats['num_matches']} \"\n",
    "              f\"| mean_err={stats['best_error']:.3f} | iters={stats['iterations']}\")\n",
    "\n",
    "print(\"\\nHomography estimation completed for all scenes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4be66-7fd2-41b6-9b74-58dfaa159315",
   "metadata": {},
   "source": [
    "In the homography estimation stage, we aimed to compute a robust projective transformation matrix \n",
    "ùêª\n",
    "H that maps points from one image onto another. We began by implementing the Direct Linear Transform (DLT) algorithm from first principles, which constructs a homogeneous system of linear equations using at least four point correspondences and solves it through Singular Value Decomposition (SVD). To improve numerical stability, we normalized all coordinates using Hartley normalization before computing \n",
    "ùêª\n",
    "H. However, since real-world image correspondences are often contaminated by noise and mismatches, we embedded this DLT procedure within a RANSAC framework to make the estimation robust to outliers. RANSAC iteratively selects random subsets of four correspondences, estimates a provisional homography, computes the reprojection error in both forward and backward directions (symmetric error), and retains the model with the largest inlier count under a 3-pixel inlier threshold. Once the best hypothesis is found, the homography is recomputed using all inliers to refine accuracy. Across all six scenes, this approach produced strong results: for instance, in the v_bird scene, the pair (1.png vs 2.png) achieved 731 inliers out of 945 matches with an average reprojection error of 1.27 pixels, while v_boat achieved 1766 inliers out of 2152 matches with a similar mean error around 1.58. In more challenging sequences such as v_graffiti, performance dropped as texture and parallax increased, sometimes leading to RANSAC failure when too few consistent correspondences existed. The inlier‚Äìoutlier visualizations clearly demonstrate that RANSAC effectively prunes erroneous matches, isolating coherent geometric relationships between images and substantially stabilizing homography estimation. This robust model forms the geometric foundation for the next stage‚Äîimage warping and panorama construction‚Äîwhere consistent transformations are critical for seamless alignment and blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b09bb1d-ac15-4b18-8dfe-ba9ceb1a95a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_bird: 1.png vs 2.png | panoramas saved | inliers=731/945 | mean_err=1.265\n",
      "v_bird: 1.png vs 3.png | panoramas saved | inliers=288/500 | mean_err=1.701\n",
      "v_bird: 1.png vs 4.png | panoramas saved | inliers=251/442 | mean_err=1.617\n",
      "v_bird: 1.png vs 5.png | panoramas saved | inliers=150/240 | mean_err=1.291\n",
      "v_bird: 1.png vs 6.png | panoramas saved | inliers=10/24 | mean_err=1.276\n",
      "v_boat: 1.png vs 2.png | panoramas saved | inliers=1766/2152 | mean_err=1.579\n",
      "v_boat: 1.png vs 3.png | panoramas saved | inliers=1303/1595 | mean_err=1.584\n",
      "v_boat: 1.png vs 4.png | panoramas saved | inliers=659/915 | mean_err=1.515\n",
      "v_boat: 1.png vs 5.png | panoramas saved | inliers=392/562 | mean_err=1.548\n",
      "v_boat: 1.png vs 6.png | panoramas saved | inliers=217/401 | mean_err=1.639\n",
      "v_circus: 1.png vs 2.png | panoramas saved | inliers=1549/1666 | mean_err=1.202\n",
      "v_circus: 1.png vs 3.png | panoramas saved | inliers=263/506 | mean_err=1.470\n",
      "v_circus: 1.png vs 4.png | panoramas saved | inliers=81/160 | mean_err=1.311\n",
      "v_circus: 1.png vs 5.png | panoramas saved | inliers=490/1115 | mean_err=1.499\n",
      "v_circus: 1.png vs 6.png | panoramas saved | inliers=166/363 | mean_err=1.335\n",
      "v_graffiti: 1.png vs 2.png | panoramas saved | inliers=734/977 | mean_err=1.162\n",
      "v_graffiti: 1.png vs 3.png | panoramas saved | inliers=153/379 | mean_err=1.580\n",
      "v_graffiti: 1.png vs 4.png | panoramas saved | inliers=17/52 | mean_err=1.630\n",
      "v_graffiti: 1.png vs 5.png | panoramas saved | inliers=6/26 | mean_err=0.000\n",
      "v_graffiti: 1.png vs 6.png | RANSAC failed: RANSAC failed to find a valid homography (insufficient inliers).\n",
      "v_soldiers: 1.png vs 2.png | panoramas saved | inliers=103/167 | mean_err=1.689\n",
      "v_soldiers: 1.png vs 3.png | panoramas saved | inliers=93/139 | mean_err=1.399\n",
      "v_soldiers: 1.png vs 4.png | panoramas saved | inliers=69/109 | mean_err=1.607\n",
      "v_soldiers: 1.png vs 5.png | panoramas saved | inliers=43/64 | mean_err=1.447\n",
      "v_soldiers: 1.png vs 6.png | panoramas saved | inliers=111/163 | mean_err=1.724\n",
      "v_weapons: 1.png vs 2.png | panoramas saved | inliers=1363/1854 | mean_err=1.401\n",
      "v_weapons: 1.png vs 3.png | panoramas saved | inliers=495/637 | mean_err=1.579\n",
      "v_weapons: 1.png vs 4.png | panoramas saved | inliers=1464/1643 | mean_err=1.317\n",
      "v_weapons: 1.png vs 5.png | panoramas saved | inliers=844/1088 | mean_err=1.508\n",
      "v_weapons: 1.png vs 6.png | panoramas saved | inliers=412/652 | mean_err=1.731\n",
      "\n",
      "Panorama construction completed for all scenes.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# IMAGE WARPING & PANORAMA CONSTRUCTION (ALL SCENES)\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from modules.feature_extraction import extract_features\n",
    "from modules.feature_matching import match_features\n",
    "from modules.homography import ransac_homography\n",
    "from modules.warping_stitching import (\n",
    "    warp_into_reference,\n",
    "    overlay_preview,\n",
    "    copy_blend,\n",
    "    average_blend,\n",
    "    feather_blend,\n",
    "    save_images\n",
    ")\n",
    "\n",
    "DATA_ROOT = \"data/panorama_dataset\"\n",
    "OUTPUT_ROOT = \"outputs/panoramas\"\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "# Reproducibility params (keep same as estimation stage)\n",
    "RATIO_THRESHOLD = 0.75\n",
    "CROSS_CHECK = True\n",
    "RANSAC_ITERS = 4000\n",
    "RANSAC_THRESH = 3.0\n",
    "CONFIDENCE = 0.995\n",
    "SEED = 1337\n",
    "\n",
    "scenes = [\"v_bird\", \"v_boat\", \"v_circus\", \"v_graffiti\", \"v_soldiers\", \"v_weapons\"]\n",
    "\n",
    "for scene in scenes:\n",
    "    scene_dir = os.path.join(DATA_ROOT, scene)\n",
    "    out_scene = os.path.join(OUTPUT_ROOT, scene)\n",
    "    os.makedirs(out_scene, exist_ok=True)\n",
    "\n",
    "    ref_path = os.path.join(scene_dir, \"1.png\")\n",
    "    ref_img = cv2.imread(ref_path)\n",
    "    kp1, desc1, _ = extract_features(ref_path, method=\"SIFT\")\n",
    "\n",
    "    for img_name in sorted(os.listdir(scene_dir)):\n",
    "        if not img_name.lower().endswith(\".png\") or img_name == \"1.png\":\n",
    "            continue\n",
    "\n",
    "        tgt_path = os.path.join(scene_dir, img_name)\n",
    "        tgt_img = cv2.imread(tgt_path)\n",
    "        kp2, desc2, _ = extract_features(tgt_path, method=\"SIFT\")\n",
    "\n",
    "        # Match and estimate H (re-run for self-containment; you can swap to cache later)\n",
    "        good_matches, all_matches = match_features(\n",
    "            desc1, desc2, method=\"SIFT\",\n",
    "            ratio_thresh=RATIO_THRESHOLD, cross_check=CROSS_CHECK\n",
    "        )\n",
    "        if len(good_matches) < 4:\n",
    "            print(f\"{scene}: 1.png vs {img_name} | insufficient matches ({len(good_matches)})\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            H, inlier_mask, stats = ransac_homography(\n",
    "                kp1, kp2, good_matches,\n",
    "                max_iters=RANSAC_ITERS,\n",
    "                inlier_threshold=RANSAC_THRESH,\n",
    "                confidence=CONFIDENCE,\n",
    "                seed=SEED,\n",
    "                use_symmetric_error=True\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            print(f\"{scene}: 1.png vs {img_name} | RANSAC failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Warp into reference plane and build outputs\n",
    "        ref_canvas, warped_tgt, overlap_mask, _ = warp_into_reference(ref_img, tgt_img, H)\n",
    "        overlay = overlay_preview(ref_canvas, warped_tgt, alpha=0.5)\n",
    "\n",
    "        # Blend variants\n",
    "        copy_pano    = copy_blend(ref_canvas, warped_tgt)\n",
    "        average_pano = average_blend(ref_canvas, warped_tgt, overlap_mask)\n",
    "        feather_pano = feather_blend(ref_canvas, warped_tgt, feather_width=30)\n",
    "\n",
    "        base = f\"1_{img_name[:-4]}\"\n",
    "        # Save standard set with feather as primary \"panorama.png\"\n",
    "        save_images(out_scene, base, ref_canvas, warped_tgt, overlap_mask, overlay, feather_pano)\n",
    "\n",
    "        # Also save comparison variants\n",
    "        cv2.imwrite(os.path.join(out_scene, f\"{base}_copy_blend.png\"), copy_pano)\n",
    "        cv2.imwrite(os.path.join(out_scene, f\"{base}_avg_blend.png\"), average_pano)\n",
    "        cv2.imwrite(os.path.join(out_scene, f\"{base}_feather_blend.png\"), feather_pano)\n",
    "\n",
    "        print(f\"{scene}: 1.png vs {img_name} | panoramas saved | \"\n",
    "              f\"inliers={stats['num_inliers']}/{stats['num_matches']} | \"\n",
    "              f\"mean_err={stats['best_error']:.3f}\")\n",
    "\n",
    "print(\"\\nPanorama construction completed for all scenes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a631962-fb28-4243-9626-943fa6f81e98",
   "metadata": {},
   "source": [
    "Report: Image Warping and Panorama Construction\n",
    "In this stage, the objective was to geometrically align the image pairs using the previously estimated homographies and to construct seamless panoramas. For each scene, the homography matrix \n",
    "ùêª\n",
    "H obtained from the RANSAC-based estimation was used to warp the secondary image onto the coordinate system of the reference image. The warping process was implemented through a perspective transformation that reprojects the corners of the target image into the reference frame, automatically computing the necessary canvas size to accommodate both images without cropping. A translation matrix was applied to shift the coordinate system such that all warped pixels lie within positive coordinates. The warped results confirm that the homography mapping correctly projects the planar regions ‚Äî for example, in the v_weapons scene, the second image aligns its tilted wall plane precisely onto the first image‚Äôs perspective, producing the expected angular overlap.\n",
    "\n",
    "To visualize alignment accuracy, we generated overlays with semi-transparent blending, which highlight both consistent and mismatched regions. Overlapping regions were extracted as binary masks to clearly identify where both images contribute valid pixels. Three different blending strategies were then applied to create final panoramas: (1) copy blending, where the warped image simply overwrites overlapping regions; (2) simple averaging, where pixel intensities from both images are averaged within the overlap to soften seams; and (3) feather blending, which uses a distance-transform-based weighting function to gradually blend intensities across the overlap, producing smoother transitions between exposures. Feather blending generally yielded the most visually coherent panoramas, as it preserved sharpness while minimizing visible intensity jumps near image boundaries.\n",
    "\n",
    "Across all six scenes, our framework automatically produced the warped image, overlay, overlap mask, and the final panorama for each image pair. Although some challenging scenes like v_graffiti exhibited alignment failure due to insufficient inliers or parallax distortions, the majority ‚Äî including v_boat, v_circus, and v_weapons ‚Äî produced geometrically consistent panoramas with well-aligned textures and minimal ghosting. These results demonstrate that our geometric warping and blending pipeline effectively constructs wide-field mosaics from hand-held or partially overlapping views, establishing a robust base for subsequent applications such as augmented reality overlay and multi-view image compositing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a96f21-521a-461a-8c60-3ff25fc7d2f6",
   "metadata": {},
   "source": [
    "Discussion on Blending Techniques\n",
    "\n",
    "To evaluate the visual quality of our panoramas, we compared three blending strategies‚Äîcopy, average, and feather blending‚Äîacross all datasets. Copy blending, while computationally simple, produced the most noticeable seams because it directly overwrote pixel values from the warped image onto the reference, often amplifying illumination or exposure differences between views. The average blending approach slightly improved this by taking the mean intensity of overlapping regions, effectively softening hard transitions but sometimes introducing visible ghosting where misalignment occurred. Feather blending, on the other hand, yielded the most natural and continuous panoramas. Its gradual weight transition‚Äîcomputed via distance transforms‚Äîassigned higher influence to pixels near the center of each image and less near borders, thereby fading exposures smoothly across overlap boundaries. This method effectively reduced ghosting and brightness artifacts, especially in scenes like v_boat and v_weapons, where exposure and geometry differences were significant. However, in extremely challenging cases such as v_graffiti, where parallax or insufficient correspondences prevented perfect alignment, even feather blending could not fully conceal geometric distortions. Overall, feather blending provided the best perceptual balance between sharpness and smoothness, confirming its effectiveness for large-scale image stitching where lighting and viewpoint variations are unavoidable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649aa45-8314-445e-8086-2e3d88bbc7bd",
   "metadata": {},
   "source": [
    "Summary and Connection to Augmented Reality\n",
    "\n",
    "The successful implementation of image warping and panorama stitching establishes a critical foundation for later tasks such as augmented reality overlay. By accurately computing and applying homographies, we demonstrated the ability to map one image plane onto another, preserving geometric consistency across different viewpoints. The feather blending approach not only enhanced visual continuity but also validated that our warping pipeline can ha<ndle exposure differences and slight alignment errors gracefully. In the AR stage, this same homography computation will be reused to project virtual elements onto real-world surfaces in a temporally consistent manner. The reliability and precision achieved during panorama construction thus ensure that any virtual object inserted into a scene remains spatially coherent with the camera‚Äôs motion, enabling seamless integration of synthetic content into real imagery. This continuity between geometric alignment, warping, and compositing defines the technical bridge between classical image stitching and dynamic augmented reality applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43d777c-2de2-4f70-936a-dbb2480dcb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR video written to: outputs/ar\\ar_dynamic_result.mp4\n",
      "Total frames: 641, FPS: 30.00\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# AUGMENTED REALITY PIPELINE \n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from modules.feature_extraction import extract_features\n",
    "from modules.feature_matching import match_features\n",
    "from modules.homography import ransac_homography\n",
    "\n",
    "# ---------- PATH SETUP ----------\n",
    "AR_ROOT = \"data/ar_dataset\"\n",
    "COVER_PATH = os.path.join(AR_ROOT, \"cv_cover.jpg\")\n",
    "BOOK_PATH  = os.path.join(AR_ROOT, \"book.mov\")\n",
    "SRC_PATH   = os.path.join(AR_ROOT, \"ar_source.mov\")\n",
    "\n",
    "OUT_DIR = \"outputs/ar\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_VIDEO = os.path.join(OUT_DIR, \"ar_dynamic_result.mp4\")\n",
    "SAMPLE_DIR = os.path.join(OUT_DIR, \"samples\")\n",
    "os.makedirs(SAMPLE_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- PARAMETERS ----------\n",
    "FRAME_STEP = 1\n",
    "MAX_FRAMES = None\n",
    "RATIO_THRESHOLD = 0.75\n",
    "CROSS_CHECK = True\n",
    "RANSAC_ITERS = 2000\n",
    "RANSAC_THRESH = 3.0\n",
    "CONFIDENCE = 0.995\n",
    "SEED = 1337\n",
    "\n",
    "# ---------- FLEXIBLE FEATURE EXTRACTION ----------\n",
    "def extract_features_flexible(image_input, method=\"SIFT\"):\n",
    "    \"\"\"\n",
    "    Accepts either a file path or a BGR numpy array for feature extraction.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    from modules.feature_extraction import extract_features\n",
    "\n",
    "    if isinstance(image_input, str):\n",
    "        # Regular path-based input\n",
    "        return extract_features(image_input, method=method)\n",
    "    elif isinstance(image_input, np.ndarray):\n",
    "        # Direct video frame\n",
    "        gray = cv2.cvtColor(image_input, cv2.COLOR_BGR2GRAY)\n",
    "        if method.upper() == \"SIFT\":\n",
    "            sift = cv2.SIFT_create()\n",
    "            kp, desc = sift.detectAndCompute(gray, None)\n",
    "        elif method.upper() == \"ORB\":\n",
    "            orb = cv2.ORB_create()\n",
    "            kp, desc = orb.detectAndCompute(gray, None)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        return kp, desc, gray\n",
    "    else:\n",
    "        raise TypeError(\"extract_features_flexible expects a str path or np.ndarray image\")\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def center_crop_resize(img, target_w, target_h):\n",
    "    \"\"\"Crop center of img to match target aspect ratio and resize.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    target_ar = target_w / float(target_h)\n",
    "    src_ar = w / float(h)\n",
    "\n",
    "    if src_ar > target_ar:\n",
    "        new_w = int(target_ar * h)\n",
    "        x0 = (w - new_w) // 2\n",
    "        crop = img[:, x0:x0 + new_w]\n",
    "    else:\n",
    "        new_h = int(w / target_ar)\n",
    "        y0 = (h - new_h) // 2\n",
    "        crop = img[y0:y0 + new_h, :]\n",
    "    return cv2.resize(crop, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def warp_and_composite(frame_bgr, src_bgr, H):\n",
    "    \"\"\"Warp src_bgr onto frame_bgr using homography H.\"\"\"\n",
    "    Hf, Wf = frame_bgr.shape[:2]\n",
    "    warped = cv2.warpPerspective(src_bgr, H, (Wf, Hf), flags=cv2.INTER_LINEAR)\n",
    "    mask = np.ones(src_bgr.shape[:2], dtype=np.uint8) * 255\n",
    "    warped_mask = cv2.warpPerspective(mask, H, (Wf, Hf), flags=cv2.INTER_NEAREST)\n",
    "    warped_mask_3 = cv2.cvtColor(warped_mask, cv2.COLOR_GRAY2BGR)\n",
    "    inv = (255 - warped_mask_3)\n",
    "    comp = (frame_bgr & inv) | (warped & warped_mask_3)\n",
    "    return comp, warped, warped_mask\n",
    "\n",
    "# ---------- PRECOMPUTE COVER FEATURES ----------\n",
    "cover_bgr = cv2.imread(COVER_PATH)\n",
    "assert cover_bgr is not None, f\"Cannot read cover at {COVER_PATH}\"\n",
    "cover_h, cover_w = cover_bgr.shape[:2]\n",
    "kp_cover, desc_cover, _ = extract_features_flexible(cover_bgr, method=\"SIFT\")\n",
    "\n",
    "# ---------- READ SOURCE VIDEO ----------\n",
    "src_cap = cv2.VideoCapture(SRC_PATH)\n",
    "assert src_cap.isOpened(), f\"Cannot open {SRC_PATH}\"\n",
    "src_frames = []\n",
    "while True:\n",
    "    ret, f = src_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    src_frames.append(f)\n",
    "src_cap.release()\n",
    "assert len(src_frames) > 0, \"No frames read from ar_source.mov\"\n",
    "\n",
    "# ---------- READ TARGET (BOOK) VIDEO ----------\n",
    "book_cap = cv2.VideoCapture(BOOK_PATH)\n",
    "assert book_cap.isOpened(), f\"Cannot open {BOOK_PATH}\"\n",
    "\n",
    "fps = book_cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "Wf = int(book_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "Hf = int(book_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Prepare writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(OUT_VIDEO, fourcc, fps / FRAME_STEP, (Wf, Hf))\n",
    "\n",
    "frame_idx = 0\n",
    "src_idx = 0\n",
    "written = 0\n",
    "\n",
    "# ---------- MAIN LOOP ----------\n",
    "while True:\n",
    "    ret, frame = book_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if frame_idx % FRAME_STEP != 0:\n",
    "        frame_idx += 1\n",
    "        continue\n",
    "    if MAX_FRAMES is not None and written >= MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "    kp_book, desc_book, _ = extract_features_flexible(frame, method=\"SIFT\")\n",
    "\n",
    "    # Match and compute homography\n",
    "    good, _all = match_features(\n",
    "        desc_cover, desc_book,\n",
    "        method=\"SIFT\",\n",
    "        ratio_thresh=RATIO_THRESHOLD,\n",
    "        cross_check=CROSS_CHECK\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        H, inlier_mask, stats = ransac_homography(\n",
    "            kp_cover, kp_book, good,\n",
    "            max_iters=RANSAC_ITERS,\n",
    "            inlier_threshold=RANSAC_THRESH,\n",
    "            confidence=CONFIDENCE,\n",
    "            seed=SEED,\n",
    "            use_symmetric_error=True\n",
    "        )\n",
    "    except RuntimeError:\n",
    "        writer.write(frame)\n",
    "        frame_idx += 1\n",
    "        written += 1\n",
    "        continue\n",
    "\n",
    "    # Prepare source frame\n",
    "    src_bgr_full = src_frames[src_idx % len(src_frames)]\n",
    "    src_idx += 1\n",
    "    src_prepped = center_crop_resize(src_bgr_full, cover_w, cover_h)\n",
    "\n",
    "    composed, warped_src, warped_mask = warp_and_composite(frame, src_prepped, H)\n",
    "\n",
    "    # Save debug samples\n",
    "    if written in (0, 30, 60, 120):\n",
    "        cv2.imwrite(os.path.join(SAMPLE_DIR, f\"t{written:04d}_frame.jpg\"), frame)\n",
    "        cv2.imwrite(os.path.join(SAMPLE_DIR, f\"t{written:04d}_warped.jpg\"), warped_src)\n",
    "        cv2.imwrite(os.path.join(SAMPLE_DIR, f\"t{written:04d}_mask.png\"), warped_mask)\n",
    "        cv2.imwrite(os.path.join(SAMPLE_DIR, f\"t{written:04d}_composed.jpg\"), composed)\n",
    "\n",
    "    writer.write(composed)\n",
    "    frame_idx += 1\n",
    "    written += 1\n",
    "\n",
    "book_cap.release()\n",
    "writer.release()\n",
    "print(f\"AR video written to: {OUT_VIDEO}\\nTotal frames: {written}, FPS: {fps / FRAME_STEP:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd751f4-6601-4b48-8128-1d564c5a3346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
